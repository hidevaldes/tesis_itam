{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_sp = get_stop_words('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def stemm_text(text):\n",
    "    \"\"\"\n",
    "    Stemm text using nltk stemmer (Snowball) for spanish\n",
    "    :param text:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    stemm_sp = SnowballStemmer('spanish')\n",
    "    tokens = word_tokenize(text, \"spanish\")\n",
    "    stems = [stemm_sp.stem(token) for token in tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#carga de pdfs limpios \n",
    "pdfs_limpios_1=pickle.load(open(r'C:\\Users\\HIDEv\\OneDrive\\Documentos\\ArchivosTesis\\pdfs_limpios.pkl','rb'))\n",
    "pdfs_limpios_2=pickle.load(open(r'C:\\Users\\HIDEv\\OneDrive\\Documentos\\ArchivosTesis\\pdfs_limpios_epn_amlo.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfs_limpios=pdfs_limpios_1+pdfs_limpios_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_sp = get_stop_words('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras_extra=['mexican','par','mexic','nuestr','mas','par','tod','pais']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_sp.extend(palabras_extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Config del tfidf vectorizer, normalizacion del texto con el metodo unicode, quitar acentos, stopwords en espa√±ol\n",
    "tf_idf = TfidfVectorizer(strip_accents='unicode', lowercase=True, stop_words=stop_words_sp, analyzer=\"word\", tokenizer=stemm_text, max_features=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Config del Count vectorizer\n",
    "cv = CountVectorizer(strip_accents='unicode', lowercase=True, stop_words=stop_words_sp, analyzer=\"word\", tokenizer=stemm_text, max_features=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HIDEv\\anaconda3\\envs\\itam_md\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estareis', 'estari', 'estariais', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'estuvier', 'estuvies', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habeis', 'habi', 'habiais', 'habr', 'habreis', 'habri', 'habriais', 'hast', 'hayais', 'hem', 'hub', 'hubier', 'hubies', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'ser', 'sereis', 'seri', 'seriais', 'si', 'sid', 'siend', 'sobr', 'som', 'suy', 'tambi', 'tant', 'ten', 'tendr', 'tendreis', 'tendri', 'tendriais', 'teneis', 'teng', 'tengais', 'teni', 'teniais', 'tien', 'tuv', 'tuvier', 'tuvies', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "build_analyzer = tf_idf.build_analyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfs_limpios_stemm = []\n",
    "\n",
    "for i in range(len(pdfs_limpios)):\n",
    "    pdfs_limpios_stemm.append(' '.join(build_analyzer(pdfs_limpios[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(pdfs_limpios_stemm,open(r'C:\\Users\\HIDEv\\OneDrive\\Documentos\\ArchivosTesis\\pdfs_limpios_stemm_final.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = tf_idf.fit_transform(pdfs_limpios)\n",
    "#x = m1.transform(content_limpio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tf_idf,open(r'C:\\Users\\HIDEv\\OneDrive\\Documentos\\ArchivosTesis\\tfidf_vect_final.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = cv.fit_transform(pdfs_limpios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.44583592, 0.04575102, 0.04381642, ..., 0.05402477, 0.0771169 ,\n",
       "         0.05959033],\n",
       "        [0.00737591, 0.00249333, 0.0422181 , ..., 0.04142234, 0.09742638,\n",
       "         0.03409923],\n",
       "        [0.        , 0.        , 0.02893743, ..., 0.04357383, 0.14227571,\n",
       "         0.09886956],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.05444893, ..., 0.03617158, 0.06806117,\n",
       "         0.08847952],\n",
       "        [0.        , 0.        , 0.01508909, ..., 0.        , 0.09053454,\n",
       "         0.04526727],\n",
       "        [0.        , 0.        , 0.0279025 , ..., 0.02965796, 0.07440667,\n",
       "         0.04650417]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(m1,open(r'C:\\Users\\HIDEv\\OneDrive\\Documentos\\ArchivosTesis\\tfidf_final.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(m2,open(r'C:\\Users\\HIDEv\\OneDrive\\Documentos\\ArchivosTesis\\cv_final.pkl','wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itam_md",
   "language": "python",
   "name": "itam_md"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
